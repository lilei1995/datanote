# python数据分析—关联分析

 



## 何为关联规则学习

关联规则学习是一种基于规则的机器学习方法，用于发现大型数据库中变量之间的有趣关系。关联分析通过寻找最能够解释数据变量之间关系的规则， 来找出大量多元数据集中有用的关联规则，它是从大量数据中发现多种数据之间的关系的一种方法；

可以基于时间序列对多种数据间的关系进行挖掘。

常用的算法为：**Apriori**，FP-Growth，PrefixSpan，SPADE，AprioriAll，Apriori-Some等

应用用捆绑销售，库存管理，商品促销设计，页面促销设计，货架设计，商品陈列设计，页面内容排版，推荐系统，商品价格策略和基于购买的用户特征分析等

网站分析工具Webtrekk中的关联分析报表便应用了关联规则管理。



频繁项集：经常出现在一块的物品的集合

频繁规则：关联结果中支持度和置信度都比较高的规则，而有效规则指的是关联规则

关联规则：暗示两种物品之间可能存在很强的关系，真正的能促进规则中的前后项的提升。





支持度

置信度

提升度（lift）指的是应用关联规则和不应用产生结果的比例

### 相同维度下的关联分析

（1）网站页面浏览关联分析

（2）广告流量关联分析

（3）用户关键字搜索关联分析

### 跨纬度的关联分析

（1）不同场景下的关联分析

（2）相同场景下的事件关联，发生的事情在同一个场景下，但属于不同的时间点。

## python代码实现

apriori模块：

```
# -*- coding: utf-8 -*-

from numpy import *
import re

def createData(fileName):
    mat = []
    req = re.compile(r',')
    fr = open(fileName)
    content = fr.readlines()
    for line in content:
        tem = line.replace('\n','').split(',')
        mat.append(tem)
    return mat

def loadDataSet():
    return [[1, 3, 4], [2, 3, 5], [1, 2, 3, 5], [2, 5]]

# C1 是大小为1的所有候选项集的集合

def createC1(dataSet):
    C1 = []
    for transaction in dataSet:
        for item in transaction:
            if not [item] in C1:
                C1.append([item]) #store all the item unrepeatly

​```
C1.sort()
#return map(frozenset, C1)#frozen set, user can't change it.
return list(map(frozenset, C1))
​```

def scanD(D,Ck,minSupport):
    ssCnt={}
    for tid in D:
        for can in Ck:
            if can.issubset(tid):
                #if not ssCnt.has_key(can):
                if not can in ssCnt:
                    ssCnt[can]=1
                else: ssCnt[can]+=1
    numItems=float(len(D))
    retList = []
    supportData = {}
    for key in ssCnt:
        support = ssCnt[key]/numItems   #compute support
        if support >= minSupport:
            retList.insert(0,key)
        supportData[key] = support
    return retList, supportData

#total apriori
def aprioriGen(Lk, k): #组合，向上合并
    #creates Ck 参数：频繁项集列表 Lk 与项集元素个数 k
    retList = []
    lenLk = len(Lk)
    for i in range(lenLk):
        for j in range(i+1, lenLk): #两两组合遍历
            L1 = list(Lk[i])[:k-2]; L2 = list(Lk[j])[:k-2]
            L1.sort(); L2.sort()
            if L1==L2: #若两个集合的前k-2个项相同时,则将两个集合合并
                retList.append(Lk[i] | Lk[j]) #set union
    return retList

#apriori
def apriori(dataSet, minSupport = 0.5):
    C1 = createC1(dataSet)
    D = list(map(set, dataSet)) #python3
    L1, supportData = scanD(D, C1, minSupport)#单项最小支持度判断 0.5，生成L1
    L = [L1]
    k = 2
    while (len(L[k-2]) > 0):#创建包含更大项集的更大列表,直到下一个大的项集为空
        Ck = aprioriGen(L[k-2], k)#Ck
        Lk, supK = scanD(D, Ck, minSupport)#get Lk
        supportData.update(supK)
        L.append(Lk)
        k += 1 #继续向上合并 生成项集个数更多的
    return L, supportData

#生成关联规则

# 创建关联规则

def generateRules(fileName, L, supportData, minConf=0.7):  # supportData是从scanD获得的字段
    bigRuleList = []
    for i in range(1, len(L)):  # 只获得又有2个或以上的项目的集合
        for freqSet in L[i]:
            H1 = [frozenset([item]) for item in freqSet]
            if (i > 1):
                rulesFromConseq(fileName, freqSet, H1, supportData, bigRuleList, minConf)
            else:
                calcConf(fileName, freqSet, H1, supportData, bigRuleList, minConf)
    return bigRuleList

# 实例数、支持度、置信度和提升度评估

def calcConf(fileName, freqSet, H, supportData, brl, minConf=0.7):
    prunedH = []
    D = fileName
    numItems = float(len(D))
    for conseq in H:
        conf = supportData[freqSet] / supportData[freqSet - conseq]  # 计算置信度
        if conf >= minConf:
            instances = numItems * supportData[freqSet]  # 计算实例数
            liftvalue = conf / supportData[conseq]  # 计算提升度
            brl.append((freqSet - conseq, conseq, int(instances), round(supportData[freqSet], 4), round(conf, 4),
                        round(liftvalue, 4)))  # 支持度已经在SCAND中计算得出
            prunedH.append(conseq)
    return prunedH

# 生成候选规则集

def rulesFromConseq(fileName, freqSet, H, supportData, brl, minConf=0.7):
    m = len(H[0])
    if (len(freqSet) > (m + 1)):
        Hmp1 = aprioriGen(H, m + 1)
        Hmp1 = calcConf(fileName, freqSet, Hmp1, supportData, brl, minConf)
        if (len(Hmp1) > 1):
            rulesFromConseq(fileName, freqSet, Hmp1, supportData, brl, minConf)

```

### apriori算法实现:

```
import apriori
import pandas as pd
from graphviz import Digraph

# 设置最小支持度阈值

minS = 0.5

# 设置最小置信度阈值

minC = 0.7

data = apriori.loadDataSet()

# 计算符合最小支持度的规则

L, suppdata = apriori.apriori(data, minSupport=minS)

# 计算满足最小置信度规则

rules = apriori.generateRules(data, L, suppdata, minConf=minC)

### 关联结果评估###

model_summary = 'data record: {1} \nassociation rules count: {0}'  # 展示数据集记录数和满足阈值定义的规则数量
print (model_summary.format(len(rules), len(data)))  # 使用str.format做格式化输出
df =  pd.DataFrame(rules,  columns=['item1',  'itme2',  'instance',  'support',
    'confidence', 'lift'])  # 创建频繁规则数据框
df_lift = df[df['lift'] > 1.0]  # 只选择提升度>1的规则
df_lift.sort_values('instance', ascending=False)

```

![1569412607054](C:\Users\83759\AppData\Roaming\Typora\typora-user-images\1569412607054.png)

#### 关联结果图形展示
```
dot = Digraph()  # 创建有向图
graph_data = df_lift[['item1', 'itme2', 'instance']]   # 切分画图用的前项、后项和实例数数据
for each_data in graph_data.values:  # 循环读出每条规则
    node1, node2, weight = each_data  # 分割每条数据画图用的前项、后项和实例数
    node1 = str(node1).strip('frozenset({})')  # 转化为字符串
    node2 = str(node2).strip('frozenset({})')  # 转化为字符串
    label = '%s' % weight  # 创建一个标签用于展示实例数
    dot.node(node1, node1, shape='record')  # 增加节点（规则中的前项）
    dot.edge(node1, node2, label=label, constraint='true')  # 增加有向边
dot.render('apriori', view=True)  # 保存规则为pdf文件

```

![1569412737395](C:\Users\83759\AppData\Roaming\Typora\typora-user-images\1569412737395.png)

### 使用pyfpgrowth模块实现FP-Growth：

```
import pyfpgrowth
数据
data = [[1, 2, 5],
        [2, 4],
        [2, 3],
        [1, 2, 4],
        [1, 3],
        [2, 3],
        [1, 3],
        [1, 2, 3, 5],
        [1, 2, 3]]

# 设置支持度和置信度

minS = 0.2
minC = 0.7

# 计算支持值

support = minS*len(data)

# 获取符合支持度规则数据

patterns = pyfpgrowth.find_frequent_patterns(data, support)

# 获取符合置信度规则数据

rules = pyfpgrowth.generate_association_rules(patterns, minC)

rules
—————————
```

结果如下：

{(5,): ((1, 2), 1.0),
(1, 5): ((2,), 1.0),
(2, 5): ((1,), 1.0),
(4,): ((2,), 1.0)}

## 序列模式的关联规则：

序列模式相较于普通关联模式最大的区别是不同的事件之间具有明显的时间区隔，以及先后的序列发生关系，能得到类似于“完成某个事件之后会在特定的时间周期内完成其他事件”的结论，例如购买了冰箱的客户会在3个月内购买洗衣机的结论。这是一种预测性分析的模式，能够将事件发生的时间和对象提取出来，使用与基于时间序列的数据化运营需求。

#### 应用场景：

基于用户上一次购买时间和商品信息，推断用户下一次购物时间和购买的商品，如果用户脱离周期过久需要实施挽留措施
基于用户上一次浏览页面的时间和页面信息，推断用户下一次可能浏览的页面
通过上衣此关键字的搜索预测下一次最可能搜索的关键字

#### 能实现序列模式的关联算法包括：

AprioriAll：基于哈希树的序列关联算法，它与Apriori算法的执行过程是一样的，不同点在于候选集的产生，需要区分最后两个元素的前后顺序。
AprioriSome：该算法是对AprioriAll算法的改进。
CARMA（Continuous Association Rule Mining Algorithm）:CARMA是一种比较新的关联规则算法，能够处理在线连续交易流数据。
GSP（Generalized Sequential Patterns）：基于水平存储结构和哈希树遍历操作的序列关联算法，它具有类似于Apriori算法的实现步骤，主要区别在于产生候选序列模式。
SPADE（Sequential PAttern Discovery using Equivalence classes）：基于垂直存储结构和格理论连接操作的序列关联算法，它是一种改进的GSP算法。
FreeSpan：频繁模式投影的序列关联算法，利用频繁项递归地将序列数据库投影到更小的投影数据库集中，在每个投影数据库中生成子序列片断，是一种分治思想的算法。
PrefixSpan（Prefix-Projected Pattern Growth）：基于前缀树的序列关联算法，从Free-Span中推导演化而来


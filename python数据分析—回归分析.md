

# python数据分析—回归分析

回归分析是研究自变量x对因变量y影响的一种数据分析方法。

主要是进行预测和控制，例如，计划制定、KPI制定、目标制定等方面，也可以基于预测的数据与实际数据进行对比和分析。

常用的算法有线性回归，二项式回归，对数回归，指数回归，核SVM，岭回归，LASSO等。

### 回归分析相关系数

回归方程一般为y=ax+b的形式，其中a为变量x的回归系数，相关系数为R，判定系数为即R2的平方

b,常数项，是回归直线在纵坐标轴上的截距

回归系数a：其绝对值大小能说明自变量与因变量之间的变化比例  （回归直线的斜率）
判定系数：自变量对因变量的方差解释，为回归平方和与总离差平方和之比值
相关系数：也称解释系数，衡量变量间的相关程度，其本质是线性相关性的判断

那么如何得到最佳的a和b，使得尽可能多的（X，Y）数据点落在或者更加靠近这条拟合出来的直线上，**最小二乘法**就是一个较好的计算方法。

最小二乘法，又称最小平方法，通过最小化误差的平方和寻找数据的最佳函数匹配。最小二乘法名字的缘由有两个：一是要将误差最小化；二是将误差最小化的方法是使误差的平方和最小化。最小二乘法在回归模型上的应用，就是要使得观测点和估计点的距离的平方和达到最小，使得尽可能多的（X，Y）数据点或者更加靠近这条拟合出来的直线上。

ps：

如果有多个自变量的话R2代表的这两个自变量共同影响的结果。假如在线性回归中只有一个自变量，那么判定系数等于相关系数的平方。
回归系数和相关系数大于0，正相关；小于0，负相关。

##### 回归分析的步骤

- 根据预测目标，确定自变量和因变量
- 绘制散点图，确定回归模型类型
- 估计模型参数，建立回归模型
- 对回归模型进行检验
- 利用回归模型进行预测

##### sklearn建模流程

- 建立模型

  - lrModel = sklearn.linear _model.LinearRegression()

- 训练模型

  - lrModel.fit(x,y)

- 模型评估

  - lrModel.score(x,y)

- 模型预测

  - lrModel.predict(x)

  ## 2.案例实操

  > 下面我们来看一个案例，某金融公司在多次进行活动推广后记录了活动推广费用及金融产品销售额数据，如下表所示

  ![@数据分析-jacky](https://img-blog.csdn.net/20171227170337927?watermark/2/text/aHR0cDovL2Jsb2cuY3Nkbi5uZXQvamFja3lfemh1eXVhbmx1/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70/gravity/SouthEast)

  > 因为活动推广有明显效果，现在的需求是投入60万的推广费，能得到多少的销售额呢？这时我们就可以使用简单线性回归模型去解决这个问题，下面，我们用这个案例来学习，如何进行简单线性回归分析；

  - ##### 根据预测目标，确定自变量和因变量

    - 问题：投入60万的推广费，能够带来多少的销售额？

  - ##### 绘制散点图，确定回归模型类型

    - 根据前面的数据，画出自变量与因变量的散点图，看看是否可以建立回归方程，在简单线性回归分析中，我们只需要确定自变量与因变量的相关度为强相关性，即可确定可以建立简单线性回归方程，我们很容易就求解出推广费与销售额之间的相关系数是0.94，也就是具有强相关性，从散点图中也可以看出，二者是有明显的线性相关的，也就是推广费越大，销售额也就越大

  - ##### 估计模型参数，建立回归模型

    最小二乘法又称最小平方法，通过最小化误差的平方和寻找数据的最佳直线，这个误差就是实际观测点和估计点间的距离；

  - ##### 对回归模型进行检验

    - 回归方程的精度就是用来表示实际观测点和回归方程的拟合程度的指标，使用判定系数来度量。
    - ![1568706613228](C:\Users\83759\AppData\Roaming\Typora\typora-user-images\1568706613228.png)

  - ![1568706630353](C:\Users\83759\AppData\Roaming\Typora\typora-user-images\1568706630353.png)

  - ##### 模型预测

    > 调用模型的predict方法，这个就是使用sklearn进行简单线性回归的求解过程；

  ![@数据分析 -jacky](https://img-blog.csdn.net/20171228102448874?watermark/2/text/aHR0cDovL2Jsb2cuY3Nkbi5uZXQvamFja3lfemh1eXVhbmx1/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70/gravity/SouthEast)

  > 解释：判定系数等于相关系数R的平方用于表示拟合得到的模型能解释因变量变化的百分比，R平方越接近于1，表示回归模型拟合效果越好
  > 

```
import numpy
from pandas import read_csv
from matplotlib import pyplot as plt
from sklearn.linear_model import LinearRegression

data = read_csv(
    'file:///Users/apple/Desktop/jacky_1.csv',encoding='GBK'
)

#画出散点图，求x和y的相关系数
plt.scatter(data.活动推广费,data.销售额)

data.corr()

#估计模型参数，建立回归模型
'''
(1) 首先导入简单线性回归的求解类LinearRegression
(2) 然后使用该类进行建模，得到lrModel的模型变量
'''

lrModel = LinearRegression()
#(3) 接着，我们把自变量和因变量选择出来
x = data[['活动推广费']]
y = data[['销售额']]

#模型训练
'''
调用模型的fit方法，对模型进行训练
这个训练过程就是参数求解的过程
并对模型进行拟合
'''
lrModel.fit(x,y)
#回归方程的精度就是用来表示实际观测点和回归方程的拟合程度的指标，使用判定系数来度量。
#对回归模型进行检验
lrModel.score(x,y)

#利用回归模型进行预测,调用模型的predict方法，这个就是使用sklearn进行简单线性回归的求解过程
lrModel.predict([[60],[70]])

#查看截距,如果需要获取到拟合出来的参数各是多少，可以使用模型的intercept属性查看参数a(截距)，使用coef属性查看参数b
alpha = lrModel.intercept_[0]

#查看参数
beta = lrModel.coef_[0][0]

alpha + beta*numpy.array([60,70])
```

- 多重线性回归（Multiple Linear Regression）
  - 研究一个因变量与多个自变量间线性关系的方法
  - 在实际工作中，因变量的变化往往受几个重要因素的影响，此时就需要用2个或2个以上的影响因素作为自变量来解释因变量的变化，这就是多重线性回归;

# 多重线性回归模型

## 1.模型

![1568703855924](C:\Users\83759\AppData\Roaming\Typora\typora-user-images\1568703855924.png)



偏回归系数

- 多重线性模型中包含多个自变量，它们同时对因变量y发生作用，如果要考察一个自变量对因变量y的影响，就必须假设其他自变量保持不变；因此，多重线性模型中的回归系数称为偏回归系数，偏回归系数β_1是指在其他自变量保持不变的情况下，自变量x_1每变动一个单位，引起的因变量y的平均变化；β_2到β_n依次类推；

- > 下面，jacky通过一个金融场景的案例，开始我们的分享：某金融公司打算新开一类金融产品，现有9个金融产品的数据，包括用户购买金融产品的综合年化利率，以及公司收取用户的佣金（手续费）；如下表所示，产品利率为11％，佣金为50，我们需要预测这款金融产品的销售额

  | 产品编号 | 百分比利率 | 抽取用户佣金 | 金融产品销售额 |
  | -------- | ---------- | ------------ | -------------- |
  | 1        | 9          | 75           | 500            |
  | 2        | 7          | 30           | 370            |
  | 3        | 7          | 20           | 375            |
  | 4        | 5          | 30           | 270            |
  | 5        | 6          | 0            | 360            |
  | 6        | 7          | 21           | 379            |
  | 7        | 8          | 50           | 440            |
  | 8        | 6          | 20           | 300            |
  | 9        | 9          | 60           | 510            |
  | 10       | 11         | 50           | ？             |

- ##### 根据预测目标，确定自变量和因变量

  - 因变量：销售额
  - 自变量：利率、佣金

- ##### 绘制散点图，确定回归模型类型

  - 从散点图和相关系数结果表可以看出，产品利率和销售额是强正相关；佣金与销售额是强负相关；因此，我们可以使用多重线性模型来解决这个问题；

> 我们对自变量和因变量绘制散点图，因为需要绘制多个变量两两之间的散点图，在这里介绍一个更先进的绘图方法scatter_matrix：我们把自变量和因变量从data中选取出来，然后设置好对应的参数。第一个是图片的大小，如果变量太多，我们就要把图片的尺寸设置的足够大才能够展示出来；第二个参数diagonal是变量与变量本身的绘图方式，我们选择kde,是绘制直方图，这个参数是什么意思，我们执行代码就知道了

- ##### 估计模型参数，建立回归模型

  - 多重线性回归模型参数的估计方法与简单线性回归模型参数的估计方法是相同的：都是采用最小二乘法进行估计（对最小二乘法更详细的解析，请参见Python回归分析五步曲（一）—简单线性回归）

```
#---author:朱元禄---
import pandas
data = pandas.read_csv(
    'file:///Users/apple/Desktop/jacky_1.csv',encoding='GBK'
)

import matplotlib
from pandas.tools.plotting import scatter_matrix
font = {
    'family':'SimHei'
}
matplotlib.rc('font',**font)

scatter_matrix(
    data[["百分比利率","抽取用户佣金","金融产品销售额"],
    figsize =(10,10),diagonal = 'kid'
)

data[["百分比利率","抽取用户佣金","金融产品销售额"]].corr()
x = data[["百分比利率","抽取用户佣金"]]
y = data[["金融产品销售额"]]

#建模
from sklearn.linear_model import LinearRegression
lrModel = LinearRegression()

#训练模型
lrModel.fit(x,y)

#预测
lrModel.predict([11,50])

#查看参数
lrModel.coef_

#查看截距
lrModel.intercept_
```

